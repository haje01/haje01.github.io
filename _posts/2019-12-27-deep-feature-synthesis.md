---
layout: post
title: "심층 특성 합성: 데이터 과학 노력의 자동화를 위하여"
description: 
date: 2019-12-27
tags: [paper,study,draft]
---

원문: [심층 특성 합성: Towards Automating Data Science Endeavors](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=10&cad=rja&uact=8&ved=2ahUKEwiA9ZqgztXmAhUDE4gKHYhrCDgQFjAJegQIBBAC&url=http%3A%2F%2Fwww.jmaxkanter.com%2Fstatic%2Fpapers%2FDSAA_DSM_2015.pdf&usg=AOvVaw1DpTqBAt1xvpH8gmQzuhBB)

주의:
- 좋은 논문을 알리기 위해 핵심 내용 중심으로 번역한 것이다.
- 모든 내용에 대한 저작권은 본문의 저자에게 있다.
- 번역에 누락이나 오류가 있을 수 있으니, 정확한 내용은 꼭 원문을 참고하기 바란다.

--- 

## I. 소개

데이터 과학은 데이터에서 통찰력, 지식 및 예측 모델을 도출하는 것으로 구성됩니다. 이 노력에는 한쪽 끝의 청소와 큐레이터, 다른 쪽 끝에 결과의 전파가 포함되며, 데이터 수집 및 동화도 포함될 수 있습니다. 데이터를 효율적으로 저장, 검색 및 처리 할 수있는 시스템 및 소프트웨어의 성공적인 개발과 확산 후, 이제는 예측적 또한 상관 관계적 분석으로 관심이 옮겨졌습니다. 우리의 목표는 이러한 노력을 보다 효율적이고 즐겁고 성공적으로 만드는 것입니다.

먼저, 우리는 KAGGLE에서 발표 한 것과 같은 많은 데이터 과학 문제와 컨퍼런스 내 경진대회(KDD 컵, IJCAI, ECML) 같은 많은 데이터 과학 문제가 몇 가지 공통된 속성을 가지고 있음을 관찰했습니다. 첫째, 데이터는 구조적이며 관계적이며 일반적으로 관계형 링크가 있는 테이블 세트로 표시됩니다. 둘째, 데이터는 복잡한 시스템과 인간 사이 상호작용의 일부 측면을 캡처합니다. 셋째, 제시된 문제는 인간 행동, 의사 결정 또는 활동의 일부 측면을 예측하려고 시도합니다(예 : 판매 후 고객이 다시 구매할지 여부 [IJCAI], 프로젝트가 기부자에 의해 자금을 지원 받을지 여부 예측 [KDD Cup 2014], 또는 택시 기사가 갈 수있는 곳 [ECML]).

예측 문제가 주어지면, 데이터 과학자는 먼저 특성(Feature)으로 알려진 변수를 만들어야 합니다. 데이터 과학자는 테이블에서 일부 정적 필드 (예 : 성별, 연령 등)를 특성으로 사용하여 시작한 다음, 직감을 통해 결과를 예측할 수 있을 것 같은 특화된 특성을 구성할 수 있습니다. 다음으로 과학자는 원시 필드를 다른 척도 값으로 변환하는 새로운 특성을 개발할 수 있습니다 (예 : "어떤 특성의 백분위수"). 이것은 그림 1의 첫 세 블록에 설명되어 있습니다.

![그림 1](/assets/2019-12-27-19-33-32.png)

> 그림 1. 전형적인 데이터 과학 노력. 분석가는 질문을 제기하면서 시작합니다: x 또는 y가 z와 상관되어 있는지 예측할 수 있습니까? 이러한 질문은 일반적으로 데이터를 보유한 비즈니스 또는 연구원의 요구에 따라 정의됩니다. 둘째, 예측 문제가 주어지면 데이터 엔지니어는 설명 변수를 가정하고 해당 변수를 추출하는 스크립트를 작성합니다. 이러한 변수 / 특성 및 표현을 고려할 때 기계 학습 연구원은 주어진 예측 목표에 대한 모델을 작성하고, 다양한 모델링 기술을 적용합니다. 현재 예측 문제에 대한 가장 일반적인 방법을 식별하기 위해 모델링 방법의 공간 내에서 선택을 합니다. 데이터 과학자는 모델을 작성, 반복 및 검증하기 위해 질문을 가정하고 변수를 형성하는 전체 프로세스를 거칠 수 있습니다.

## II. 심층 특성 합성

*심층 특성 합성(심층 특성 합성)*은 관계형 데이터 세트에 대한 기능을 자동으로 생성하는 알고리즘입니다. 이 알고리즘은 기본적으로 데이터의 기본 필드에 대한 관계를 따른 다음, 최종 특성을 만들기 위해 해당 경로를 따라 수학 함수를 순차적으로 적용합니다. 계산을 순차적으로 쌓아가면서, 우리는 각각의 새로운 특성이 특정 깊이 $d$를 갖는 것으로 정의할 수 있다는 것을 알았습니다. 따라서 우리는 알고리즘을 *심층 특성 학습*이라고 부릅니다. 이 섹션에서는 심층 특성 학습의 동기를 설명하고, 특성 합성 추상화를 정의하고, 알고리즘을 제시합니다.

### A. 프로토타입 문제와 동기

전자상거래 웹 사이트에 대한 가상의 데이터 세트를 고려해 봅시다. 우리의 목표는 사용 가능한 모든 데이터를 기반으로 고객의 행동을 설명하는 특성을 계산하는 것입니다. 이 데이터 세트의 스키마는 그림 2에 표시되어 있습니다.

![그림 2](/assets/2019-12-28-21-03-26.png)

> 그림 2. 전자상거래 웹 사이트를 위한 단순화된 스키마. 4 개의 엔터티가 있습니다. 한 엔터티에서 다른 엔터티로의 화살표는 데이터베이스에서 첫 번째 엔터티가 두 번째 엔터티를 참조 함을 나타냅니다.

데이터 과학자처럼 생각하기 위해, 우리는 고객을 설명하는 특성(예 : "고객이 얼마나 자주 구매합니까?" 또는 "고객이 마지막으로 구매 한 후 얼마나 되었습니까?")으로 번역 될 수 있는 질문을 하는 것으로 시작할 수 있습니다. 또한 고객과 관련된 엔터티를 보고 이에 대해 질문 할 수도 있습니다(예 : "고객의 총 주문 가격은 얼마입니까?" 또는 "이 고객은 일반적으로 고급 또는 저렴한 제품을 구매합니까?"). 이러한 질문을 관계를 따르고, 값을 집계하고, 새 특성을 계산하는 것을 통해 특성으로 전환할 수 있습니다. 우리의 목표는 이러한 유형의 특성을 생성하거나, 프록시 수량으로 동작할 수 있는 계산을 생성 할 수 있는 알고리즘을 설계하는 것입니다.

### B. 특성 합성 추상화

심층 특성 합성에 대한 입력은 상호 연결된 엔터티의 집합 및 그것과 연관된 테이블입니다. 테이블에 있는 엔터티의 각 인스턴스마다 고유한 식별자가 있습니다. 선택적으로 엔터티는 관련 엔터티의 고유 식별자를 사용하여 그 인스턴스를 참조할수 있습니다. 엔터티의 인스턴스에는 숫자(numeric), 범주 형(categorical), 타임 스탬프(timestamp) 및 자유 텍스트(freetext) 타입 중 하나에 해당하는 특성이 있습니다.

주어진 데이터 세트의 표기 측면에서, 우리는 $E^{(1 ... K)}$로 엔터티를 나타냅니다. 여기서 각 엔터티 테이블은 $1 ... J$ 특성을 가집니다. 특정 항목을 $x^k_{i, j}$로 표시하는데, 이는 $k$ 번째 엔터티의 $i$ 번째 인스턴스에 대한 특성 $j$의 값입니다.

다음으로 우리는 엔터티와 그것의 데이터 테이블 그리고 관계 측면에서, *엔터티* 레벨과 *관계* 레벨에서 서로 다른 두 가지 레벨로 적용되는 여러 수학 함수를 정의합니다. 우리는 아래에서 이 함수들을 살펴볼 것입니다. 특성 조립의 대상인 엔터티 $E^k$를 생각해 봅시다. 표기상의 편의를 위해 특정 엔터티를 나타내는 데 사용되는 $k$를 생략하겠습니다.

첫 번째 특성 세트는 엔터티 $k$에만 해당하는 테이블의 특성 및 그 값을 고려하여 계산됩니다. 이것을 엔터티 특성이라고하며 아래에 설명합니다.

엔터티 특성 (`efeat`) : 엔터티 특성은 각 항목 $x_{i,j}$의 값을 계산하여 유도합니다. 이 특성은 배열 $x_{:,j}$에 요소별로 적용된 계산 함수를 기반으로 할 수 있습니다. 예를 들어 범주 형 문자열 데이터 유형을 사전 결정된 고유 숫자 값으로 변환하거나 숫자 값의 반올림과 처럼, 엔터티 테이블의 기존 특성을 다른 타입의 값으로 변환하는 함수가 있습니다. 다른 예로는 타임 스탬프를 주중일(weekday) (1-7), 월중일(day of the month) (1-30 / 31), 연중월(month of the year) (1-12) 또는 일중시(hour of the day) (1-24)의 4 가지 특성으로 변환하는 것이 있습니다.

이러한 특성에는 $j$ 번째 특성 $x_{:, j}$ 및 다음 식으로 주어지는 $x_{i,j}$에 대한 전체 값 세트에 함수를 적용하는 것도 포함됩니다.

![식 1](/assets/2019-12-29-08-19-16.png)

이러한 계산의 예로는 누적 분포 함수 (cdf) 기반 특성이 있습니다. 이 특성을 생성하기 위해 $x_{:,j}$에 대한 밀도 함수를 만들고, $x_{i,j}$ (또는 *백분위수*)의 누적 밀도 값을 평가하여 새로운 특성을 형성합니다.

두 번째 기능 세트는 두 개의 관련 엔터티 $E^l$ 및 $E^k$를 결합 분석하여 유도됩니다. 이 두 엔터티는 *순방향(forward)* 또는 *역방향(backward)* 의 두 가지 방법 중 하나로 관련됩니다.

순방향 : *순방향* 관계는 엔터티 $E^l$의 인스턴스 $m$과 $E^k$의 다른 엔터티 $i$의 단일 인스턴스 사이에 있습니다. $i$가 $m$에 대한 명시적인 의존성이 있기 때문에 이것은 *순방향* 관계로 간주됩니다. 위의 전자상거래 예에서, *Orders* 엔터티는 *Customers*와 순방향 관계를 갖습니다. 즉, *Orders* 테이블의 각 주문은 한 고객과 만 관련이 있습니다.

역방향 : 역방향 관계는 $E^k$의 인스턴스 $i$에서 순방향 관계를 갖는 $E^l$의 모든 인스턴스 $m = \{1...M\}$ 로의 관계입니다. 위와 같은 예에서 *Customers* 엔터티는 *Orders*와 역방향 관계가 있습니다. 즉, 많은 주문이 동일한 고객을 가리킬 수 있습니다.

직접 특성 (`dfeat`) : 직접 기능은 *순방향* 관계에 적용됩니다. 이들에서, 관련 엔터티 $i \in E^k$의 특성은 $m \in E^l$에 대한 특성으로 직접 변형됩니다.

관계형 특성 (`rfeat`) : 관계형 특성은 *역방향* 관계에 적용됩니다. 그것들은 $x^l_{:,j \vert e^k=i}$에 수학 함수를 적용하여 엔터티 $E^k$의 인스턴스 $i$에 대해 유도됩니다. 이것은 관련 엔터티 $E^l$의 특성 $j$에 대한 값의 모음이며, $E^k$의 식별자가 $e^k = i$ 인 엔터티 $E^l$의 특성 $j$의 모든 값을 추출하여 결합됩니다.

![식 2](/assets/2019-12-29-08-37-03.png)

`rfeat` 함수의 일부 예는 `min`, `max` 및 `count`입니다. 다른 `rfeat` 함수는 $x^l_{:,j \vert e^k=i}$에 대한 확률 밀도 함수에 적용될 수 있는 함수를 포함합니다.

![그림 4](/assets/2019-12-30-22-57-15.png)

### C. 심층 특성 합성 알고리즘

심층 특성 합성 알고리즘을 설명하기 위해 먼저 $E^{1 ... K}$로 표시된 $K$ 엔터티의 데이터 세트을 고려합니다. 우리의 목표는 목표 $E^k$에 대한 `rfeat`, `dfeat` 및 `efeat` 함수를 추출하는 것입니다. 또한 우리는 $E^k$가 *순방향* 또는 *역방향* 관계를 갖는 모든 엔터티를 알고 있습니다. 이들은 세트 $E_F$ 및 $E_B$로 표시됩니다.

먼저, 엔터티 내에 이미 존재하는 특성을 사용하여 `efeat` 함수가 작성되는 것을 볼 수 있습니다. `rfeat` 및 `dfeat` 함수를 먼저 합성해야 결과에 `efeat` 함수를 적용 할 수 있습니다. $E^k$에 대한 `rfeat` 함수를 생성하기 위해 $E_B$의 엔터티 특성을 사용합니다. 따라서 우리는 $E^k$의 `rfeat` 함수를 실현하기 전에 $E^B$의 각 엔터티에 대한 특성 타입을 모두 만들어야합니다. 비슷한 방식으로 $E^k$에 `dfeat` 함수를 추가합니다. 이러한 특성은 $E_F$ 엔터티의 기능을 사용하여 실현되므로 $E_F$의 각 엔터티에 대한 모든 특성을 먼저 계산해야합니다. 마지막으로, 모든 `rfeat` 및 `dfeat` 함수가 $E^k$에 추가되면 `efeat` 함수를 생성할 수 있습니다. 그림 3은 각 특성 유형을 올바르게 생성하기 위한 계산 순서를 보여줍니다.

![그림 3](/assets/2019-12-29-20-15-12.png)

> 각 특성 유형을 합성 할 때 계산 제약 조건의 그림. `rfeat` 및 `dfeat` 함수은 독립적으로 합성 할 수 있는 반면, `efeat` 함수은 `rfeat` 및 `dfeat` 함수에 따라 다릅니다. 심층 특성 합성에 대한 접근법의 한 가지 인스턴스화는 알고리즘 1에 나와 있습니다.

다음으로 대상 엔터티와 관련된 엔터티에 자체 관련 엔터티가 있는 시나리오를 고려합니다. 이 경우를 처리하기 위해 위에서 설명한 동일한 순서를 사용하여 특성을 재귀적으로 생성 할 수 있습니다. 재귀는 특정 깊이에 도달하거나 관련 엔터티가 없을 때 종료될 수 있습니다.

![알고리즘 1](/assets/2019-12-29-20-18-56.png)

위에 있는 `MAKE_FEATURES`의 알고리즘 의사 코드는 $i$ 번째 엔터티에 대한 특성 $F^i$를 만듭니다. 재귀 호출의 구성 및 각 특성 유형의 계산은 위에서 설명한 제약 조건을 따릅니다. 의사 코드의 `RFEAT`, `DFEAT` 및 `EFEAT` 함수는 제공된 입력을 기반으로 각 기능 유형을 합성합니다. 알고리즘은 나중에 합성된 기능의 사용을 돕기 위해 정보를 저장하고 반환합니다. 이 정보에는 특성 값뿐만 아니라 적용된 기본 특성 및 특성에 대한 메타 데이터도 포함됩니다.

우리 알고리즘에서는 `ffeat` 함수과 `rfeat` 함수 사이 제약이 없더라도, `ffeat` 전에 `rfeat`을 계산하도록 선택합니다. 또한 $E_V$는 우리가 "방문한" 엔터티를 추적합니다. 10 행은 방문한 엔터티에 대한 `dfeat` 함수가 포함되지 않도록 합니다. 이것은 불필요한 특성을 계산하지 않도록 합니다. 예를 들어, 전자상거래 데이터베이스 예에서 *Customer* 엔터티의 특성을 작성하는 경우를 고려하십시오. 주문한 고객의 연령에 따라 각 주문에 대해 `dfeat`을 작성하는 것은 이치에 맞지 않습니다. 나중에 *Customers*를 위해 `rfeat`를 만들 때 주문한 고객을 기준으로 각 주문을 집계하고, 각 그룹의 주문은 `dfeat` 함수에 대해 동일한 값을 갖기 때문입니다.

그림 4는 재귀적으로 생성되는 특성의 예를 보여줍니다. 이 예에서는 결국 모든 고객의 평균 주문 크기를 계산합니다. 그러나 이 값을 실현하기 위해 *Product* 엔터티부터 시작하여 중간 계산을 수행합니다. 먼저, 제품 가격을 *ProductOrders* 엔터티에 추가하기 위해 `dfeat` 함수를 계산합니다. 다음으로 *Orders* 엔터티의 특정 인스턴스와 관련된 모든 *ProductOrders* 인스턴스에 `SUM` 함수를 적용하여 주문에 대한 `rfeat` 함수를 계산합니다. 마지막으로, 우리는 각 고객의 평균 총 주문 크기를 계산하기 위해 또 다른 `rfeat` 함수를 계산합니다.

### D. 특성 수의 증가

심층 특성 합성에서 열거할 수있는 특성 공간이 매우 빠르게 커집니다. 그것이 어떻게 성장하는지 이해하기 위해, 우리는 주어진 엔터티에 대해 알고리즘이 합성할 기능의 수 $z$를 분석합니다. 특성 합성의 재귀적 특성으로 인해 엔터티에 대해 생성 된 특성 수는 관련 엔터티에 대해 생성 된 수에 따라 다릅니다. 따라서 우리는 $i$가 되풀이되면 엔터티에 대해 생성 한 특성의 수를 $z_i$로 표시할 수 있습니다. 또한 데이터 세트의 모든 엔터티가 $O(j)$ 특성으로 시작하고 $O(n)$ 순방향 관계와 $O(m)$ 역방향 관계가 있다고 가정합니다.

먼저, $O(m)$ 엔터티에 대한 `rfeat` 유형 특성을 합성합니다. 만약 우리가 $O(r)$ `rfeat` 함수가 있다고 가정한다면, 우리는 총 $O(r \cdot z_{i-1} \cdot m)$ 부가 특성을 위해 각각의 $m$ 엔터티에 대한 $O(r \cdot z_{i-1})$ 특성을 역방향 관계로 합성합니다. 다음으로, 순방향 관계에서 엔터티의 모든 특성에 대해 하나의 `dfeat` 함수를 작성합니다. 이것은 $O (z_{i-1} \cdot n)$ 기능을 추가한다는 의미입니다. 마지막으로, 우리는 $j$ 개의 오리지널 특징과 새로 합성된 $O (z_{i-1} \cdot (r \cdot m + n))$ 특성을 사용하여 `efeat` 함수를 만들어야 합니다. 우리는 $O(e)$ `efeat` 함수가 있다고 가정합니다. 따라서 추가 $O (e \cdot j + e (z_{i-1} \cdot (r \cdot m + n)))$ `efeat` 함수를 합성합니다.

모든 `rfeat`, `dfeat` 및 `efeat` 함수를 결합하면 $z_i = O(z_{ i-1} \cdot (r \cdot m + n) (e + 1) + e \cdot j)$임을 알 수 있습니다. $z0$에서는 `efeat` 함수만 계산할 수 있으므로 $z_0 = O (e \cdot j)$입니다. $p = (r \cdot m + n) (e + 1)$ 이고 $q = e \cdot j$이라고 하면, 대체하여 다음을 얻습니다.

![](/assets/2019-12-29-22-04-40.png)

그리고 $z_{i-1}$을 $z_{i-2} \cdot p + q$로 대체합니다. 

![](/assets/2019-12-29-22-05-39.png)

$z_0$까지 확장을 계속하면

![](/assets/2019-12-29-22-06-26.png)

위의 방정식에서 $z_0 = O (e \cdot j) = q$를 대입하면 다음과 같습니다.

![](/assets/2019-12-29-22-06-52.png)

따라서, $z_i$에 대한 닫힌 형태는

![](/assets/2019-12-29-22-07-12.png)

## III. 심층 특성 합성: 구현

심층 특성 합성을 구현하면서, 우리는 신속하게 데이터 과학 장치를 배포하고 새로운 데이터 세트가 발생할 때 합성된 특성을 평가하는 것을 목표로 했습니다.

데이터 과학 장치와 함께 제공되는 심층 특성 합성 알고리즘은 테이블용 InnoDB 엔진을 사용하여 MySQL 데이터베이스 위에 구축됩니다. 모든 원시 데이터 세트는 데이터 과학 장치에서 처리하기 위해 수동으로 MySQL 스키마로 변환되었습니다. 파이썬에서 합성된 특성을 계산, 관리 및 조작하기 위한 로직을 구현합니다.

데이터를 저장하는 방법과 심층 특성 합성의 요구 사항 사이의 자연스러운 병렬 관계로 인해 관계형 데이터베이스를 사용하기로 선택했습니다. 각 항목에서 엔터티는 테이블로 표시되고 특성은 열로 표시됩니다.

모든 수학 함수는 특성 함수 인터페이스를 제공해야합니다. 특성 함수는 특성 유형에 따라 하나 또는 두 개의 엔터티를 입력으로 사용합니다. 관계형 레벨 특성 및 소스가 다른 엔터티인 직접 특성의 경우 두 개의 엔터티가 필요합니다. 함수 정의는 관계에서 적용 가능한 열을 결정하고 출력 값을 계산하는 방법을 책임집니다. 

성능상의 이유로 데이터 과학 장치는 MySQL에서 제공하는 기능 위에 특성 함수를 빌드합니다. 현재 데이터 과학 장치는 `AVG()`, `MAX()`, `MIN()`, `SUM()`, `STD()` 및 `COUNT()`와 같은 `rfeat` 함수를 구현합니다. 또한 시스템은 텍스트 필드의 문자 수를 계산하는 `length()` 및 날짜를 ​​발생한 요일 또는 월로 변환하는 `WEEKDAY()` 및 `MONTH()`와 같은 `efeat` 함수를 구현합니다. 단순함에도 불구하고 이 작은 기능 기반은 데이터 과학 장치 평가에 사용할 수 있는 광범위한 기능을 작성하기에 충분합니다.

필터 객체는 `rfeat` 함수에 대한 데이터의 하위 집합을 선택할 수 있는 유연한 방법을 제공합니다. "and" 또는 "or"과 같은 논리 연산자를 사용하여 기존 필터 개체를 결합하여 새로운 필터 개체를 만들 수 있습니다. 필터 개체는 필터링되는 열을 반환하는 인터페이스와 MySQL 쿼리에 직렬화하는 방법을 제공합니다.

필터 객체는 심층 특성 합성에 유용한 두 가지 기능을 구현합니다. 첫째, 특정 조건이 참인 경우에만 `rfeat` 함수를 적용하는 방법을 제공합니다. 우리는 이 사용법을 범주형 필터라고 부릅니다. 예를 들어 "X 회사가 제품을 제조한 제품에 이 고객이 지출한 총 금액"은 범주형 필터를 구성합니다. 둘째, 날짜 필드에 상한과 하한을 지정하여 시간 간격 기능을 구성할 수 있습니다. 예를 들면 "고객이 3 월 31 일 이후, 4 월 30 일 이전에 주문한 주문의 수"입니다.

사용 중에 데이터 과학 장치는 필요할 때만 데이터베이스 조회를 구성합니다. 기능을 계산하는데 필요한 데이터는 여러 테이블에 저장될 수 있으므로 쿼리의 `SELECT` 절 또는 `WHERE` 절에 있는 모든 열에 액세스 할 수 있도록 테이블을 조인해야 합니다. 쿼리 예는 그림 5에 있습니다. 쿼리는 여러 조인과 테이블 스캔을 줄이기 위해 한 번에 여러 계산을 수행하도록 최적화되었습니다.

![그림 5](/assets/2019-12-29-22-37-42.png)

> 그림 5. KDD cup 2014 데이터 세트에서 Donors 엔티티에 대한 기능을 작성하기 위해 데이터 과학 장치에서 자동 생성된 MySQL 쿼리의 예. 이 쿼리는 각 공여자가 제공한 총 금액을 계산합니다.

## IV. 예측 기계 학습 경로

심층 특성 합성에서 만든 특성 사용하기 위해 일반화된 기계 학습 경로를 구현합니다.

첫 번째 단계는 예측 문제를 공식화하는 것입니다. 모델링할 데이터 세트의 특성 중 하나를 선택하여 이를 달성합니다. 이 예측하고자 하는 특성을 목표 값으로 부릅니다.

목표 값을 선택한 후 예측에 사용하기에 적합한 특성을 조립합니다. 이러한 특성을 *예측자(predictors)*라고 합니다. 예측자가 공통 기본 데이터를 목표 값으로 하여 계산되거나, 목표 값이 발생할 때 존재하지 않는 데이터에 의존하는 경우 유효하지 않은 것으로 필터링됩니다. 또한 데이터 과학 장치는 각 엔터티 기능과 관련된 *메타데이터* 데이터베이스를 유지 관리합니다. 이 *메타데이터*에는 특성을 구성하는 데 사용된 원래 데이터베이스의 기본 필드에 대한 정보와, 그 안에 포함된 시간 종속성이 포함되어 있습니다.

### 재사용 가능한 기계 학습 경로

대상 특성과 예측 변수를 선택하면 데이터 과학 장치는 데이터 전처리, 특성 선택, 차원 축소, 모델링 및 평가를 위한 초모수화된 경로를 구현합니다. 초모수를 조정하기 위해 데이터 과학 장치는 지능형 초모수 최적화를 수행하기 위한 도구를 제공합니다. 기계 학습 및 예측 모델 구축을 위해 다음 단계가 수행됩니다.

*데이터 전처리* : 머신 러닝 경로에 들어가기 전에 `null` 값을 제거하고, 원핫 인코딩을 사용하여 범주형 변수를 변환하고, 특성을 정규화하여 데이터를 정리합니다.

*특성 선택 및 차원 축소* : 심층 특성 합성는 엔터티 당 많은 수의 기능을 생성합니다. 형상 공간의 크기를 줄이기 위해 두 가지 기술을 순차적으로 사용합니다. 먼저, 잘린 SVD 변환을 사용하고 SVD의 구성 요소를 선택합니다. 그런 다음 목표 값에 대한 *f-value* 값을 계산하여 각 SVD 특성의 순위를 정하고, 최고 $\gamma\%$ 순위 특성을 선택합니다.

*모델링* : 모델링을 위해 $n$ 개의 의사 결정 트리를 구성하여 랜덤 포리스트를 사용합니다. 각 의사 결정 트리의 깊이는 $m_d$이며 $\beta$로 표시되는 특성의 일부를 사용합니다. 많은 데이터 세트의 경우, 여러 데이터 포인트 클러스터에 대해 별도의 모델을 갖는 것이 강력할 수 있습니다. 이를 통합하기 위해 우리는 *K-Means*를 사용해 훈련 포인트를 $k$ 클러스터로 분리하고, 각 군집에 대해 별개의 랜덤 포레스트를 훈련시킵니다. 테스트 샘플에 대한 라벨을 예측하기 위해, 훈련된 클러스터 분류기는 데이터 포인트에 클러스터 라벨을 할당한 다음 해당 모델을 적용합니다.

경우에 따라 분류 문제에서 목표 값 클래스 중 하나가 제대로 표시되지 않습니다. 이를 보완하기 위해 모델링 단계에서는 표현이 부족한 클래스를 $rr$의 계수로 다시 가중치를 지정할 수 있습니다.

모델링 단계에서는 $n$, $m_d$, $\beta$, $k$, $rr$의 네 가지 모수를 소개했습니다. 다음으로 이 경로를 자동 튜닝하는 방법을 설명합니다.

![테이블 1](assets/2019-12-30-22-56-19.png)

V. 가우시안 코폴라 프로세스를 이용해 베이지안 모수 최적화

머신 러닝 파이프 라인의 여러 단계에는 튜닝 할 수 있는 모수가 있으며, 이 튜닝은 모델 성능에 현저한 영향을 줄 수 있습니다. 순진한 그리드 검색은 가능한 모든 모수 값 조합을 고려할 경우 6 x 490 x 90 x 10 x 450 x 20 x 100 = 2,381,400,000,000 (2조 3천 8백 14억)의 가능성 공간에서 검색을 초래합니다. 이 공간의 탐색을 돕기 위해 가우시안 코풀라 프로세스(Gaussian Copula Processes, GCP)를 사용합니다. GCP는 모수 선택과 전체 경로의 성능 간의 관계 $f$를 모델링하는 데 사용됩니다 (모델). 그런 다음 새 모수를 샘플링하고 그 성능을 예측합니다 (샘플). 마지막으로 선택 전략을 적용하여 다음에 사용할 모수를 선택합니다 (최적화). 아래에서 각 단계를 자세히 설명합니다.

모델 : 일반적으로 가우시안 프로세스는 $f$가 $\chi, \{f(\tilde p_i)\}^N_{i=1}$에서 $N$ 포인트의 유한 집합이 $\mathbb R^N$ 다변량 가우시안 분포가 되도록 모델링하는 데 사용됩니다. 이러한 공정의 속성은 평균 함수 (중앙형 데이터의 경우 일반적으로 0으로 간주)와 공분산 함수 $K : P \times P \rightarrow \mathbb R$로 결정됩니다. 

매핑 함수 : [5]에서, 변환된 출력이 가우시안 프로세스에 의해 가장 잘 모델링되도록 모수화된 매핑이 학습됩니다. 그러나 이러한 매핑은 동일한 데이터 세트에 대한 많은 실험에서 다른 매핑을 학습하기에 불안정합니다. 더욱이, 유도된 일변량 분포는 거의 대부분 가우시안이며 모수화된 매핑은 더 큰 유연성을 제공할 수 없었습니다. 이를 극복하기 위해 커널 밀도 추정을 통해 관찰된 데이터에서 주변 분포를 학습하는 새로운 접근 방식을 사용합니다. 

보다 구체적으로, 파라미터 $\bar p = \{p_1 ... p_m\}$ 및 성능 함수 $f (\bar p)$를 고려하겠습니다. 변환의 첫 번째 단계는 커널 밀도 추정기를 사용하여 $\{f (\bar p_i)\}_{i = 1}^N$ 의 밀도를 모델링한 다음, 이 밀도의 *cdf*를 추정합니다. 

그런 다음 $\{f (\bar p_i)\}_{i = 1}^N$ 의 각 값에 대한 cdf 값을 생성하는데, $g=cdf(f(\bar p))$에서 제공됩니다. $g$가 표준 법선의 표본이라고 가정하면, $g$의 값에 $\psi^-1$을 적용하여 $h = \psi^-1 (g)$로 주어지는 최종값을 생성합니다. $h$ 는 정규 가우스 프로세스를 사용하여 모델링하려는 $f (\cdot)$의 변환을 나타냅니다. 따라서 가우시안 프로세스 모델링에 대한 입력 값은 $\bar p_{1 ... n}$ 및 대응하는 $h_{1 ... n}$ 값입니다.

*공분산 함수 맞추기* : [3], 119-221 페이지에서 영감을 얻은 주기적 성분을 갖는 제곱 지수의 일반화를 사용합니다. 우도 최대화를 통해 모델 모수를 학습합니다.

샘플링 : $\mathcal P$에서 반복적으로 포인트를 샘플링하고 GCP 모델을 사용하여 해당 결과 값 $f(\bar p)$를 예측한 후, 다음에 선택할 포인트를 결정합니다.

최적화 : 이 최종 단계는 일반적으로 *획득 함수* $a$를 최대화하여 수행됩니다. 이 함수에 대한 입력은 탐색(입력 공간 $mathcal P$의 탐색되지 않은 영역의 테스트 포인트)과 활용 (높은 $f (\bar p)$ 값을 예측하는 테스트 포인트) 간의 균형을 맞추기 위해 $f$에서 파생됩니다. 특히 이것으로 인하여 국소 최적값 근처 검색에 집중하지 않아도 됩니다.  그러므로 주어진 관측치 $(p_{1 ... n}, f(p_n))$에서, $\mathcal P$에서 ${\bar p}_i^\prime$을 임의 표집하고, 그것의 출력 $f_i^\prime$을 예측하고, $a$를 최대화하는 $\bar p^{\prime \star}$를 선택합니다.

VI. 실험 결과

Data Science Machine은 이런 방식 최초의 것이기에 “기계 성능은 어느 정도입니까?”, “중요한 특성을 생성했습니까?” 및 “자동화가 잘 동작합니까?" 같은 질문들을 언급하려 합니다. KDD cup 2014, IJCAI 및 KDD cup 2015와 같은 많은 데이터 과학자들이 최고의 결과를 위해 경쟁하는 데이터 세트에 적용하여 Data Science Machine의 효과를 입증합니다. 이런 각 대회에서는 수백명의 데이터 과학 팬들이 주최자들에 의해 고안된 예측 문제 풀이에 참여합니다. 그림 6의 도표는 각 데이터 세트의 엔터티를 보여줍니다. 이어서 각 문제를 간략하게 설명하겠습니다.

![그림 6](/assets/2019-12-30-22-26-53.png)

> 그림 6. 2014-2015 년 동안 왼쪽에서 오른쪽으로 KDD 컵 2014, IJCAI 및 KDD 2015 컵의 세 가지 다른 데이터 과학 경연 대회. 총 906 개의 팀이 이 대회에 참가했습니다. Data Science Machine 솔루션의 예측이 제출되었으며 결과는 이 백서에 보고됩니다.

KDD cup 2014 - *프로젝트 흥분* : DonorsChoose.org의 과거 프로젝트 기록을 사용하여 크라우드 펀딩 프로젝트가 "흥미로운" 것인지 예측하십시오.

IJCAI - *반복 구매자 예측* : 과거 판매자 및 고객 쇼핑 데이터를 사용하여 판촉 기간 동안 구매한 고객이 반복 구매자로 전환되는지 예측하십시오.

KDD cup 2015 - *학생 중퇴* : 온라인 과정에서 리소스와 학생의 상호 작용을 사용하여 향후 10일 내에 학생이 중퇴할 것인지 예측합니다.

![그림 7](/assets/2019-12-30-22-50-54.png)

> 그림 7. 세 가지 데이터 세트 모두에 대해 반복적으로 발견된 최대 교차 검증 AUC 점수. 위에서 아래로 : KDD 컵 2014, IJCAI, KDD 컵 2015

경쟁하기 위해 데이터 과학자는 특성 엔지니어링, 모델링 및 미세 조정에 관여합니다. Data Science Machine에서 이러한 세 가지 활동은 초기 모수 설정외 최소한 사람의 개입으로 모두 수행 할 수 있어 계산 한계가 최대화됩니다. 우리는 Data Science Machine에서 완전 자동화된 특성 생성 (*심층 특성 합성*)을 먼저 실행했습니다. KDD 컵 2014의 경우 시간 간격 특성도 구성했으며 IJCAI 및 KDD 컵 2015의 경우 몇 가지 범주형 필터를 만들었습니다. *심층 특성 합성*을 실행 한 결과는 표 III에 나와 있습니다.

